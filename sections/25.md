### 25. I/O Scheduling

  * There are 3 I/O scheduling schemes, one of which must be compiled into the  
    kernel:

      1. Completely Fair Queuing (**CFQ**)
      2. Deadline scheduling
      3. noop (a simple scheme)

    Modern distributions choose either **CFQ** or **Deadline**.

  * SSD drives do not require matching read/write requests in order of the location  
    of the data on the disk (**elevator** scheme), but do require **wear leveling**  
    to make best use of their limited read/write cycles.

    One can determine whether a drive is an SSD by examining  
    `/sys/block/<device>/queue/rotational`, which returns `1` for platter devices  
    and `0` for solid-state.

  * Examining scheduling scheme for a device using `sysfs`(`/dev/sda`, for example):
        $ cat /sys/block/sda/queue/scheduler
        noop deadline [cfq]

    And can be changed using the command line during runtime:
        $ echo noop > /sys/block/sda/queue/scheduler
        $ cat /sys/block/sda/queue/scheduler
        [noop] deadline cfq

    More tunable parameters are found under `/sys/block/<device>/queue/iosched`:
        $ ls -l /sys/block/sda/queue/iosched/
        total 0
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 back_seek_max
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 back_seek_penalty
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 fifo_expire_async
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 fifo_expire_sync
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 group_idle
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 low_latency
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 quantum
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 slice_async
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 slice_async_rq
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 slice_idle
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 slice_sync
        -rw-r--r-- 1 root root 4096 Mar 25 08:58 target_latency

  * The **CFQ** tries to equally spread I/O bandwidth between all processes with  
    requests. There are 64 queues which are filled with requests using a hash of  
    the PID of the requesting process.

    Dequeuing is done round-robin across the 64 queues, with each queue picking  
    requests in a FIFO fashion. They are then merged/sorted to prevent excessive  
    disk seeking before being sent to the device.

  * The **Deadline** scheduler tries to improve performance *and* prevent starvation.
    With each request a deadline is associated. Read requests get priority.

    Five separate I/O queues are maintained:
      * Two sorted lists, one for reading, one for writing, arranged by starting  
        block.
      * Two FIFO lists, one reading, one writing, sorted by time of submission.
      * Fifth queue contains requests being sent to the device (dispatch queue).

    How the requests are picked from the four pending queues and moved to the fifth  
    queue is the determines the efficiency of the algorithm.
